一、SNN概述
目前常用的CNN、RNN等神经网络是第二代神经网络，而脉冲神经网络SNN属于第三代神经网络模型，为了减小神经科学与机器学习之间的差距，SNN使用最拟合生物神经元机制的模型来计算。

二、SNN基本架构
1.SNN基本架构（三层网络）
它使用脉冲序列作为输入，最重要的是神经元的膜电位。一旦神经元到达某一电位，脉冲就出现，随后达到电位的神经元会被重置。SNN通常是稀疏连接，利用特殊的网络拓扑。

2. 脉冲网络优化目标
各种脉冲神经网络的监督算法的目标基本一致：对输入脉冲序列Si(t)和期望输出脉冲序列Sd(t)，通过监督训练脉冲神经网络，调整权值W，实现神经网络实际输出脉冲序列So(t)与Sd(t)之间的差距尽可能小。

3.脉冲网络训练步骤
1.	确定编码方式，将样本数据编码为脉冲序列
2.	将脉冲序列输入脉冲神经网络计算得输出脉冲序列
3.	将期望脉冲序列和实际输出脉冲序列对比得到误差，并根据误差调整W。

4.脉冲序列构建方法
脉冲序列构建方法是通过神经元的放电频率来传递信息。在大脑皮层，连续动作电位的时序非常没有规律。一种观点认为这种无规律的内部脉冲间隔反映了一种随机过程，因此瞬时放电频率可以通过求解大量神经元的响应均值来估计。另一种观点认为这种无规律的现象可能是由突触前神经元活动的精确巧合所形成的，反映了一种高带宽的信息传递通路。

5.脉冲神经元模型
1）HH模型
•	一组描述神经元细胞膜的电生理现象的非线性微分方程，直接反映了细胞膜上离子通道的开闭情况。
•	精确地描绘出膜电压的生物特性，能够很好地与生物神经元的电生理实验结果相吻合，但是运算量较高，难以实现大规模神经网络的实时仿真。
2）LIF模型
•	解决HH模型运算量问题，LIF模型将细胞膜的电特性看成电阻和电容的组合。
3）Izhikevich模型
•	HH模型精确度高，但运算量大。LIF模型运算量小，但牺牲了精确度。
•	Izhikevich模型结合了两者的优势，生物精确性接近HH模型，运算复杂度接近LIF模型。

6.脉冲神经网络训练方法
第二代神经网络主要基于误差反向传播原理进行有监督的训练，而对于脉冲神经网络，神经信息以脉冲序列的方式存储，神经元内部状态变量及误差函数不再满足连续可微的性质，因此传统的人工神经网络学习算法不能直接应用于脉冲神经神经网络。目前，脉冲神经网络的学习算法主要有以下几类。
1）无监督学习算法
①Hebbian Learning 赫布学习算法
•	基于赫布法则 (Hebbian Rule)，当两个在位置上临近的神经元，在放电时间上也临近的话，他们之间很有可能形成突触。而突触前膜和突触后膜的一对神经元的放电活动（spike train）会进一步影响二者间突触的强度。
•	突触可塑性：如果两个神经元同时兴奋，则它们之间的突触增强，也就是上一层发放脉冲之后，下一层相连的神经元跟着发放脉冲，那么该突触权重增加，反之该突触权重削弱。
②STDP（Spike Timing Dependent Plasticity）学习算法  ----主流算法
•	脉冲序列相关可塑性，强调发放时序不对称的重要性。突触权值自适应调整。
2）监督学习算法
①基于突触可塑性的监督学习算法
a. 监督Hebbian学习算法
•	通过信号使突触后神经元在目标时间内发放脉冲，信号可以表示为脉冲发放时间，也可以转换为神经元的突触电流形式。
•	在每个学习周期，学习过程由3个脉冲决定，包括2个突触前脉冲和1个突触后脉冲。第一个突触前脉冲表示输入信号，第二个突触前脉冲表示突触后神经元的目标脉冲。 
b. 远程监督学习算法（ReSuMe）
•	训练脉冲神经网络时，突触权值的调整仅依赖于输入输出的脉冲序列和STDP机制，与神经元模型无关，因此该算法可适用于各种神经元模型。
•	后来针对该算法的改进，可应用到多层前馈脉冲神经网络。
②基于梯度下降规则的监督学习算法
a.SpikeProp算法
•	适用于多层前馈脉冲神经网络的误差反向传播算法
•	使用具有解析表达式的脉冲反应模型(SpikeResponse Model)，并为了克服神经元内部状态变量由于脉冲发放而导致的不连续性，限制网络中所有层神经元只能发放一个脉冲。
b.Multi-SpikeProp算法
•	对SpikeProp算法改进，应用链式规则推导了输出层和隐含层突触权值的梯度下降学习规则，并将其应用到实际的Fisher Iris和脑电图的分类问题，
•	Multi-SpikeProp算法比SpikeProp算法具有更高的分类准确率。
c.Tempotron算法
•	训练目标是使得实际输出膜电位更符合期望输出膜电位，认为神经元后突触膜电位是所有与之相连的突触前神经元脉冲输入的加权和，并据此判断该输出神经元是否需要发放脉冲。
•	采用的神经元模型是LIF模型，成功实现了单脉冲的时空模式分类，但该神经元输出仅有0和1两种输出，此外它无法拓展到多层网络结构。
③基于脉冲序列卷积的监督学习算法
    通过对脉冲序列基于核函数的卷积计算，可将脉冲序列解释为特定的神经生理信号，比如神经元的突触后电位或脉冲发放的密度函数。通过脉冲序列的内积来定量地表示脉冲序列之间的相关性，评价实际脉冲序列与目标脉冲序列的误差。

5.脉冲神经网络优点
    脉冲训练增强处理时空数据的能力。空间指神经元仅与附近的神经元连接，这样它们可以分别处理输入块（类似于 CNN 使用滤波器）。时间指脉冲训练随着时间而发生，这样在二进制编码中丢失的信息可以在脉冲的时间信息中重新获取。允许自然地处理时间数据，无需像RNN添加额外的复杂度。

三、STDP应用于MNIST手写网络
1.参考文献
1)《Unsupervised learning of digit recognition using spike-timing-dependent plasticity》
•	设计了一种基于STDP的SNN无监督学习算法，学习时不需要标签，训练结束后对神经元归类即可，不需要SVM或者线性层。
•	特点：无监督，神经形态硬件兼容性，可扩展性，最先进的准确性，以及没有领域特定的知识
•	强调所设计的SNN是有可实现的硬件基础的，例如LIF神经元，电导突触，指数型动态特性，横向抑制，可调节发射阈值。
•	良好的规模伸缩性，网络的准确度是可以随规模增大提升的（但是有一定上限），100/400/1600/6400个激活型神经元对应的预测精度为82.9%/87.0%/91.9%/95.0%。
•	不需要对输入数据做预处理（除了从像素点到尖峰序列的转化）
•	所提出的学习机制与k-means和competitive learning algorithm有相似之处，每个神经元会学习到输入的一小部分模式
2) https://github.com/zxzhijia/Brian2STDPMNIST

2.STDP-MNIST整体概述----测试集个数：10000 预测准确率91.43%
STDP-MNIST  数据./data/
            网络权重值 ./weights/
            随机值 ./random/
            训练结果保存 ./activity/
            程序：训练 Diehl&Cook_spiking_MNIST_Brian2
                 预测 Diehl&Cook_MNIST_evaluation
                 修改网络结构 Diehl&Cook_MNIST_random_conn_generator
                 
3.SNN网络相关设置
1）输入层：784个神经元
2）兴奋层：400个神经元，静态电位-65mv,复位电位-65mv,静默电位-52mv,静默期5ms
3）抑制层：400个神经元，静态电位-60mv,复位电位-45mv,静默电位-40mv,静默期2ms
